# -*- coding: utf-8 -*-
"""Features_Selection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c9DDdiWg3TGMxSsed2Mjqo2u7CjoCNhE

# **Sélection des caractéristiques (Features selection)**

La sélection des caractéristiques permet de sélctionner un sous-ensemble de features dans l'ensemble des caractéristiques de la base des données. En revanche l'ingénierie des caracteristiques permet d'extraire un nouvel ensemble de caractéristiques à partir des caracteristiques existantes. Nous parlerons dans ce document que de la sélection des caractéristiques, l'ingénierie des caracteristiques fera l'objet d'un autre document. 

Le plus souvent, Il existe beacoup de caractéristqiues non pertinentes dans un ensemble de données qui ne contiennent pas beaucoup d'informations. Ces caractéristiques, lorsqu'elles ne sont pas traitées, influenent la capacité de généralisation et la performance des algorithmes d'apprentissage automatique. On parle dans ce cas de la **malédiction de la dimensionnalité**. Pour parrer à ce porblème lié à la malédiction de la dimensionnalité, nous devons sélectionner les caractéristiques qui sont les plus importantes ou qui influent le plus sur la variable cible et qui sont pas redondantes. 

Nous avons divisé la sélection de caractéristiques en trois méthodes **filter** , **wrapper** et **embedded**.

## **A. Méthodes de filtrage**

Ces méthodes sélctionnent des caractéristiques sur la base des analyses statistqiues univariées des features. En effet, on calcule un score pour chaque caractéristique et on sélectionne un sous ensemble de caractéristiques en fonction de ce score. 
L'avantage de ces méthodes est qu'elles sont plus rapides et moins coûteuses en calcul que les méthodes **Wrapper** mais l'inconvénient est qu'elles ne performent pas plus que les méthodes de Wrapper.

Dans ce qui suit, nous présentons les méthodes de filtrage les plus couramment utilisées.

**1. Gain d'information**


Le gain d'information calcule la réduction d'entropie à partir de la transformation d'un ensemble de données. Il peut être utilisé pour la sélection de caractéristiques en évaluant le gain d'information de chaque variable dans le contexte de la variable cible. En effet, il s'agit de calculer le gain d'information entre la variable cible et chaque variable d'entrée dans l'ensemble de données d'apprentissage. L'outil **sklearn** fournit le package [mutual_info_classif()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) pour le calcul du gain d'information. 

En termes simples, le [gain d'information](https://ravedata.in/machine-learning/feature-selection-ig/) est la quantité d'entropie (désordre) que nous allons supprimer en connaissant au préalable une caractéristique d'entrée. Mathématiquement, le gain d'information est défini comme suit :

IG(Y/X) = H(Y) – H(Y/X)

Où H(Y) est l'entropie de y et H(Y/X) est l'entropie conditionnel de Y sachant X

Plus le gain d'information est important, plus l'entropie est supprimée et plus la variable X contient d'informations sur Y.
"""

# Commented out IPython magic to ensure Python compatibility.
# Importer les librairies
import pandas as pd
from sklearn.feature_selection import mutual_info_classif
import matplotlib.pyplot as plt
# %matplotlib inline 
from sklearn.datasets import load_diabetes

# Charger l'ensemble des données
df= load_diabetes()
y= df.target
X= df.data

# Implementer le package et tracer le graphique visualisant l'importance de variables 
# par rapport à la variable target
importances=mutual_info_classif(X, y)
print(importances)
feat_importances=pd.Series(importances, df.feature_names[0:len(df.feature_names)])
# bar plot : https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html
feat_importances.plot(kind='bar', color='teal')
plt.show()

"""**2. Fisher Score**

Cette [technique supervisée](https://www.sciencedirect.com/science/article/abs/pii/S002002552100832X#:~:text=Extraits%20de%20section-,Mod%C3%A8le%20de%20score%20de%20Fisher,-Le%20score%20de) de sélection de caractéristiques est l'une des méthodes les plus largement utilisées. Elle a pour objectif principal de trouver un sous-ensemble des caracteristiques et de maximiser les distances entre les points de données dans différentes classes tout en minimisant les distances entre les points de données dans la même classe. 

Soit un ensemble de données par rappaort à c classes différentes ![image.png](data:image/png;base64,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), le score de Fisher de la ième caractéristique est calculé par:

![image.png](data:image/png;base64,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)

Où ![image.png](data:image/png;base64,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)est la dispersion entre classes de la ième caractéristique, ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABUAAAAWCAYAAAAvg9c4AAAChUlEQVRIidXUT0jTcRjH8fdPp0j+pluSONtw/neZYlSIIREeKjCkY6dCD0WBWJeEgqCDhyIIQ/BmHbv4P8GDw0NgHSz8gw5T58y5iX/azGnq5p4OTqdhaejFz/H5wouH58vzKCIiHHEijho8tqgQ8C0w411le8gBH/Nu9+7aQSIiIrIqLusLKUtXBbVC3k8uiuvTO3lcmiOAoJZKTc+cBOVg2ex0xcn32CvcvpEJeBntaODtsJE79VYmrTUU+Tqoaxlg6YCNagA4kU5hYTzLTREQpSO5pILy7DgUhPUfWhQM5GUnox4QDc80OI+jzwWWAs6nalEA2MA7NY6NBNIStShs4O1vpPbZQ+429LO2L+pzMTLoRp+fiiFKCRVXcdvH8GDhYlYCCpHoTCoTdU3MRsUSvR8anHHQ59ZjsRjRbZmyyJRtEgwZmJOiQ6VZHB4j+eaTKHuJYTSIzznGIKfJTztF1Narfw77wDTkZWBUIwBhxTnOV3LINWsBD1/e3CKr5DW9vuCf6DozjjHcpGAxxW93IF4nNpsHQ4GZpAiANaZHh5kyZ5CSqEF8dnqsQxgvnyMlNjxJTWigOEfsoC8mzRCzReJ3TzDg0WOOW8Xe3Y1ScIaFETsU3kTraObJ8y6SHjTTdjUDdccsNvntn0/HpIsMv2qiUVUPg60f+ByTTW7cHN967JgCLTy90ICh+iVV13aDm/38Mxvyy7MgS/7QLi10SpU+R+61fpTG8rOSeb9NXHus2T7o7gSG6qWY61LbtyTLva+kiEtSbZ2Rda9XfDvw/0ADMtteKapaKe2zAZHgtHQ+KhLIkbKaLnH5w6oicojLLyt4f2rQxe9eg8Ohf8nxufy/AXw04EIs5IBGAAAAAElFTkSuQmCC) est le nombre d'échantillons dans la kème classe, ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACUAAAAfCAYAAABgfwTIAAAEnUlEQVRYhe2YfUzUdRzH3ycPJg8KxaNyQSLMkoWo1XCsKU7G+iPckpVrMdhcZi3Uf8Rsom1ROdoK1HzYKEfUyEG4MCPGRcPGsTyeH/QkDkU4OETgut/hPfD7vftDPH7HcYuNY/WH7z8/v+/n83nd9/v9vH/7nYIk8T/Tsv8aYD49hlqoHkMtVB6EEiH0VODEicvQ2QlAxGR7JYryD+Ltr9thlS/lGJpOfYrzaj2mlw6KsOsqkLu3Dk9nbsczPgoAXghSBqD/9I8Y9fGHr3y54ils2b0F+uMH8aVmHC6eRE9IusPKnG3cXz1ISR7WlTIDyTzWcH+epGka1SeZnPw5NWbJ6YlHdkrqrcP562nISl0NhWz3pgb70IL12BATCGACzcVvID71C2gECYAXVr74KvYG1qKmw+hUzwNQIib7e3BjxybE+ylkcSuGentwN2YdosO8QUGHRlU3ol5OQrT/TNtlUUjcCTT0jECSZXovHoqwmgX4RoVgpVNcwKBWB7y0C4G3q3D0ozpEvFuFn9LWIcDBvhxhyjXou2uGhNkd8gCUG4kjuNWogzL8Mj7cMoXUP8qRmxgwZ5EC3j6+LqlL51PGIdzsDscrWbnIyryD0+frMewyZiIsZuFfoAQNzu3PwVtHrmBQVkAauIKjOTl4/4dbEOcp4R/0JGx/DWNC3m6kH80TMXgueiPS38tGyNnPUFRvgN1ohNlR2wLDwCgSwlfByx2UOKDBd+cq0BkSgVDHuYu431GLUxdb4RXknPyoROC6RKRcbUDLuOjIGdf1oC0gDmtXr4Dfpj04fog4uWMbdp/R4G9xhsrWjz+v+iH1+QjIR0TmUzYOlOcQSOSBmhGZa9xnw7FkApks0T5w41MGqvLSuadUS5tbLzNzYtIqC9hpqMljQkYJtTZnn5JB3aPq8GYCb7JUZ5F5XCfPpgQTMQVUW5yTZR1p19cyf+cu5tfdpd0dmEMWjl6/wOzkd1iqNbk8nYV61DyxkBpZc0lfyewAMPhADZ18WZpkd9UFljQMzkBItOsb+U3J79Tb3cHP5nZdKuPPfUbOt9IB9fCVMLe5RKHhOJWIZFrJDYryzOk2FiXFcHthE11/6+I0AyXSqPqAkVAyo7R3ll4yUJW3lUAKP1ZPeri1e81M3wP0d7ViGNF4IS50ZhJECB0VKDrTCGADno32B8d6cXNsGhS6cOnILsSnFkI94WoSi9VDKA6j47duAFaYpmwgRAjd5Sj4xQepGbFA8BPw9TKi5WIZ2qfsmNSNwTfSD731fRi2LMHHEEnSqOLhSCVjY4OJ2G3MzHyN+4qvUW+3Ul99iHEAASV3FjbRKJGkwLaidCKpiG3Tnj8+kBLN6k+YEFNAtWmUWk0rtfdklkAL72lbqWkfoMlx2YZYvW+960R6SN6AER2qWnRFvY6VK0IRvzl0zl4uR0j8RoTIQ9YBtP9qwIa8NVjl+cPDMo4343KZBlgbhmDXd8i84ugddN5ei61xgTAZbZ6Hmuq+hu9vCoiMCIL/ApMk0wSG0IyvCr5F89QSXHTJ1MWq4nOsnsfu3WuapmGD7I55Vgry8R8cC9I/WMjdqk9IxEgAAAAASUVORK5CYII=) est la moyenne de la ième caractéristique dans la kème classe, ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAYAAADAQbwGAAACW0lEQVQ4jeWVS0iUURiGH5U0ktCBDCcQL4NSpmaIG3EhQtDOIFy2cCUtItqUtQx140qjFIIQs6gQEhSSyFwoTsro6OjEqF1UvKRm0zCTjZf53xbp5EymBm6id/lxznPe73LOiZAkDlCRBwn7V4E+Gw2Xy7hU0c7Mtuoa0+3cKivjytNxAn8DDEzbeNTQwsixRBIiglGWHS+502gnKj6OqP0D15kb6acHC0XZScQE4x5c/TZ8ZJCTGrcL7jegh4lBB5BFXkb8NttzOLtckJJLZnIMuykUGPjEeO8HOJNL5onoYFiL4/QNuTGV5JERs1kHeXjbep8H3bNs/AmoaTsdPW5MRdmkbm1ErLxz0Okzk5+TTNC3Mcmr29U0983gD4EEFZCn86bMJKmkaULGVthYUOeNAkGhKq1ftZe2OfzOx1E78ySTn57AT38BfI4Wau/2Aqc5lRyLPk/gmhrmWcUFMoprsLpDh+gXUPM4XjuBVbwra4gAPucTql4corjEAqbDREd5GGxsote1RLT5CBNd75n3hz0FQa+eTl03J8liMQlLkUpLL6q8rltz66uaa7umdBAk6VzNG3kMn4Zqz4uztRraCE15E2jom7VaWSlVsnoXNWaza2zJv22ZX0tjdtmGp+U1JGlWbeUnZbraoWXtCHTLWlkkCuvlDDtxR/mtqkwxqbB+ROHLIwH0ZYDWZhukHce0270KzuUUI5NpFKQfxetZC2+KWHF289jlw5wYT+zePAyvm1kGuFf1kIGVHZpieEf1vK5BbWPefeQrSRvyzi9s1jNUEdL/9gX8ALHu/pGj21H4AAAAAElFTkSuQmCC) est la moyenne de la ième caractéristique dans X , ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOEAAAAoCAYAAAAIVJ0GAAAS50lEQVR4nO2deViU5d7HPwMoooiAgI6CgAguqJioHD2uuVTvcSu38niw8jW145aVlEfrlMuxUElNpU5gYlpWWEaWpKaB5hKogKLsCAImOwybzMz9/gHYDMzADIuQ73yui+vS57nv5/n97vt7/+71AYkQQmDAgIFWw6i1DTBg4P87hkZowEArY2iEBgy0MoZGaMBAK2NohAYMtAiC8vhveWvuCKyHTWXB1BG4Pb6c3WEZyGulNDRCAwZaBIG8OJscl3VEXAjhs+/P8O3cLFYu+YQLRUq1lCatZKEBA484Rph7LmavZ83/TbGys4VbWWSXKMHCSCWlAQMGHgJyigvyoJ8U207qza4JjbCCnNhfCDn4OWfSK5pooIE/H49q/beMXyL/Eof8U3nh3/P5i0UTGqGQRXN4+1fckCkAUCSH8E/v49xRm2kK5Fln2L0nnGzDWZxHDlUNPCr137Cum+aTkEWzf806wp/azOZZfWhX677ujVDkcPHDEEzmTMPd3Bhoh1nH9sgd+uBk114loQQT6VgWesbg901KnZUgA39i1DTQ4dGof5103XifhCya/StWc8z1XQ5tmITURFInjc6NUJkSir/ycab26lB9pZLcjDSyhrpg31GCMv0E76z2JTS9AjDGYsRk+h8LJqL0TxIO2wryDMJ2v8GawAiyUs4StHkNy97+mtjqKN2aqGvg0ah/3XUt19unqgboQ/iYDzj05kSkxsUkf7+dbWdy1NLp2AjLSTx7Hscxfen44FoJdxKScRjshJ1EQXFCOJ9GV2LWwbj6yQ54jUvkm4u5ur1CDSXlWdGcCTmE/zY//I+cI6UNiLDlUZB/4yqXb/yE34cbeefTZLp5OpK7YwdfXC1sZdtqa6Al6/9hoaeu9fKphOhAHxZ9eoJPF3nQ2UiCxKgLLtOu0s2ps1pKlUZYSUH8OY4G7mKzzzIWLlyGz+Zt7AoMJjzlDvGXjXFXzSzySI3OZ2ifjtz9/j8s+8qRw4deZaxtza6HKT1dpZy7noHW5iPPIGz7Fg7Fl6pcVJAf4c/zs3YS064nPSvO8vqzGzh8vbhufnGH0K17+TnrUVkYMMbKYxRedh1wmPoa2995kSc8HDCTdcHGwhSoICf+N87F5tCk/kVjuTdEnroGmqP+W508PXWtwSetGuyEx8ofEULU+vmMfzibqqWsLjEZ8V+sZ8E72czc9BJTF83EzqyU1PNB/GvxcYpDt9D1hgI71fFsaRYJV0rIl7/JVItFhB6aRy+14a4E046dqLgv1ywYUUDUvs182/cN3nf7Iw5RGsXBdb5ETTrAx0+OxWLSQCKnldFjkGXdZ0jsmbJkMFtf3YvZ9pWMtDKup8BbiKIw3vL6Gxtvgcv4J/hrr05aEpaQdj6Us0myWtfN6bfhOJfeHYsFVIsgh/7TbTED5JnJnDd3ZU6PDiBP5fR/VvHDtCOMHtBIe7WVe0Mo8rijqoGm1n9boLZP0IBfGnxqBg2agKAy/kvWLj5Gj53HeX3WgAerN9J5PuxWBpPRy4zkWhnFvVSi00cwz28cPy3+jlMJ03hR50pVkH9uD2szZvH58l4qJwYE9xPOc+SkMY8tcqAzgIkNboO1P0liNZJ/vnSBeRtPcmD7k9jVnfe2LBajWLFnDScm7qDAeQ5vfzSX3u0aMqKqRwsLPUrgzv9y/MOjhC0fxVQ7k2oRWDBkpQ1GVJKRGEvSkBH0tjYGI2Mos2SQoxXIc0m9a4qjvTm6u6yt3PWnafXfdmmMX03VoBEoyI+PJkzWlb5OdurLp5IuuM9/kSn2dti7G1Mpr2n/gtI7SVyRutFv0mxWvpTKa1tPkHm/iIISxYM0FaVlWJub1hVJaRQHNyUy78WRWKvdrOTurWtEY4mDbWcdxWWMhddsnr/7X4KutMa8yQTbCavY6zuRrP1v8Ir/FWQNhn5TbNxG88yK7Xx74RT+M6I5ei4TQY0IetPX3hwoJSMxAStPKR2KSxH375J4qTsOdiZkn3iHoWuPk65PN6O13OXIcu5RUK56nEogl+Vyt6C8KuobW6tooIn13+JUkBN/jcjIBHLkQvt1NZ+qbK7fL20+NU2DKnPCXOJS71GpMZk1biMU3EitmZfJyctMI31QH+w7d2fMi4sY+9VCxi/5howHPleQkZDFX9x7oN5BKymKDOG/XZ9kkqtqhCknPnABjs8GIiOS9yfaIpHMJTC+vGEvJA5MmO/Ark/CudcaYx+JNcOWb2bnvHZ8t24j/pF5Og7BJJjYevHSjrfp9fMl0kWNCPrg1L09UE5hdjH5AR9xIDIPRV4m8XTHJPUw2897cXLnzFpDwPrQUu6ikBuBLzPU1oURa46TVWN4UTjvDndCOsaPS6UCdQ00pf4fAiWX2TXpMYYtOsZt1Qmp4haHnx3DsNlfkagA/XVdj09N0aAQQihzTwmffuYC8yeFT/BNUawUdVAkfSa8N/8qSureEkIoRWVBgZCp5lPEi6DFu0RESe2HZYvTa0eJKQE3haLOY+6IkKUDBdI3xenCOnfrRZkcJGZYrRIhv1fqla/5UIrK28Fiqau5YPhb4vQ9feyoFNlRV0RymYaCrywU+TJ51T8jfIWLh6fwfGyLuFCnXBtCc7krs6JE+PUoEbJ2lMB8vgiIq6ph5e8hYqmVg5jse1EUVr9Kuwb0qf+WRxEXIKaAsFp1QuSqXFcmB4kZta7rpesGfGqsBo0AJNbjWf+1P0v7R/HerClMW3OQyGz11R4j56dYYXaW79M09UwSTLp0odODqKwg//wP3Jw+k6Eda4VqRQbXT4KXq13d/RFRzL3kXHCW0rWDfoMYicMARltf5nJckV75mg8JJr2m867/Gob/toOlb31HmlzXkGiCzeDHcNbks4kFlp2MATnZtxNRzFnPlqnh7ApO0DJq0YKWcpd0H8xo90FM8fZmCt/h91UUpSKHi5/sJ2lFAAdWj8Ci2iztGtCj/lscBbnx1/gVKcMHO2Kpcj0/PoqwWtd113XDPjVWg9X1YYy5+3x2h/7I56v7EfGBN+P/9joHbxSqrAJZM2zZ0/BVyIPjPZoRyLPOcezuRNb9zaHufOBeEpFXe+LUTcNkV5ZJXEwWVoOdkTa4uFEL4644DCslPrOo/qFgZSyBM52RSCQ6/nSm/1th6FasJthOWMHOzePI8n+DtUGx6DCY1pFyMpPT8OrjwfgFsyndshafL66j8yZDfeWOhPYDnmLJHHuuHwhkz7b1vFexlAO1T3jopIEG6r/FkZEUdRUZLowdIFUJOCWkxESTj5N6IGpOn3TVYJ3H1+lTC8T1gMXCFYT5hPfFr3lyvbrWhqgaKiwRwZl1u2zl7S/Ec+ZWYtTOq6JC7yeniWBvF+HiGyFaa0Bag7I4Quya7iQwnyF8f8sVD39AVpf6yr0Kucg9vV70w1y4Lg0WtyvbgtWNQHFTBEyRCqxeFSdyVbSrTBBBMxwE5itEyL3m1fQfNE6DdU/MSLrg/sIm/H1GITvzAz/HadgkbwLK4jySNIcDyjKTiZH1ZHBv2zqHXKuSFJFyKbLNn56RmA9lqd9WXpCe5vWXd3Imu/VPUGovd6iK9Bc49GUEnVwgKymD/PsPwyoF+WEbGdFZt1FJ52ErCU5p4GBGwW2if8uC4QNwtlRZPslPIjIsHUYNwa1rK+wn14PmrSKJBfZuPYFkikqbV0DG1t0YSKKGOwpkOb+TgQ32Np00dvmKm4fx/stRZkR8zWueFhpSGGHR3qT+IZCQkR4Txz19JlSdHfFws9FjX01Cu94z2bQjkgvTfiTkylImPCHVY2hWQXb0NbKkQxhsa9pwch3QXu4CedYpNi47huPmAA6NeJ1hi77g84vzGPy4nbrNooiUywngPgRnc3kz2GiM1dgNXC7e0Mj8tRFUxEdyLB+cxg7A8UEXU0nm2WME5YODlys9VbseNZ+ao3HqoMFamMT+cBazyeNwVpuDVVCcVwzmHgzto0nsjUfSxQ4nq9vcuVcBUlVZl5OVnEi+eW/69NC8OWrcfwGHb07Fqq8Gm0QFJYUdGWRvXf+SuPwesb+cIUafk24Ok+nrZoO5HlkQZWSnZNLLdzfvTNGnAQJ5Z9k8fjXlQafxn9pDx0wKZCnhfOn/AXttN3DxNU+1oKG53Ksb4MK94LMbb/ceGFs/yz9c57PnkzO8PEH9FIxaEHS+0AgbW5oKbsdeIxUXvPv3pCo0COSZofj+6yD5uDBtiBOdxO9cOpWN++SBmDUY2PVAVw3W5ovnxoulR+LEg9XxygKR/MuHwtvVUzwfdEOU1Qxc798QAc+8qT7ObgzyGLFvtKdYdeJurRvZ4vRaT8FAzcvvyuIEcWKnj3gl4Ioo1vRcWZjY4DBZ+EYUNs2+ZqFcZISsFaMaPbcqF9nxSSJb17zKdHHi7cXi+eXe4ikrNM9JNJV7WZTYN91JmM8MEkk1+xbK38Vpn1FV89mzkSL8pyiRV2OGslik3Uyv3sLS08aHwl1xYpWHAKmYvC9G3BdKUZl5Vuza8pn48ZMXBDiIGUHxojzugFhRs1Wj5lMTaaQGjbBsx8l5fbEdPpV/LJzNhH6DmLM9gUE7PmP3ggF0AITsFkc3vMqqM9H8+Ikf245EUfsEpM4Y2zPkSQvOxqSj1hkpc0i9lon5oN70MKvdbygoSLpFTOxP+P2SgqZZqjIjgUvyYQx11au/agEUyKICeHm3lJ3vzaCXhu/H6kWeycXArWz+7g7CWMe8Enue+PfH7PdbyePWWtJoKHdlZhShP+cglVryoMgltox5aSUvSMPw33GK+wP7YiUBIUskdPcm/H7NBkUjbHwYVKQQcQzGj+/CyWXPMGX2M8z3vcvEFc/yxP88zTLXfI55u2Gz9A6zp/VBoupTM7jRaA0KIYRQlon8rEyRmZklsos1rFoWx4kjqzyFlfc+cTYiQkSlFTdhxU8pSiK2iZGjdolrFSpPKflVbBoorWdltCrKOW26IMrr3CsTcQHzxMBN2jZdHxZKUZkRIlaPWi2Cb5c1nFxD/or40+Lj9dMFLr4iQt9l3soI4euipSfUWO4KUZafK4rr9Ga1r8tF3rUQ4bvkMYH31yKlKTa2IPIb+8Rolojg25kiro5OlaIyO15ERMSItGK5UPcpWGQ2+e2N12DV5EDSAcvuUjR8p1B129wMk/winDxHMtrTo4nHkCR0HDqH1/ou5fNzf0z+RU4aN1KHMm9cH9pryqbIJiWyFK/l3eveL43hu/2dWfupB615hFjIrrDv5c/ouXMvTz/4SFQHKmMJ/GcYnh8uwcPVixG29+GvzlinneSjo1Eae36A9jZePP330TjotKeqqdyN6GCpqeusfd0YK4/hDOqgxMmhJ1LXQdU29qZHm/l9fSXEhZ3knJMnvt2649ZLWuu+BBMbVzxt/rjyh0890DaA0JkmaFC3IpTfI/m8MSPm2DbPOUCJA9PWLyRk9dvMX3sNs1UfsJozxCx4ka3uWj4FKszg1o1uDHK0qjVyKCUp+ABXX1hNgEsrNkF5Gsc3bCTW24/dw6x1H92IQq4H+nJg4CoWtJcAhaTHpuIyyAmH3p4seW1y89lYXe7H1x8jdtT/4q7PqSS1IFhjoyO2zWdd0yiN5YegM2A8kna6DpHrC+z6vbxJGtTty/rs28QkOTHAoYveL9BM1RL+5oWQ2tGWspOfcrhgOsG+2udQivQ4wvJdcbG4T+GD42AKZNe/ZE/yDLZ5V81fWwVRQNS+dezu+SbvPe2sY2SrICc2lH3LpjJyaRHPTXKtEoIim+TLCv7a2w4TISM9OpLISM0/0ekyPb/Vq946WZTLjj2XKdIns2oQVLVRr/e3HIqECxy+kK9f76w1sOv15iZrUCdz5ZnJnMeRGXYmlBQUYWpp0QyFb0qPWR9w1DaYcMupzB2sy/LwKT4+co1xGybRBUCZzq9XXHj9X6M1/gKdh0MFmce38Oy6TMb5hPHx9vAGkmeTGJ9EnOoHvlMCeLxf9Qig8C7Jqd1w69GZ8tRrRCbeQ6ntWXfBVjpUT99NkU58hU3tgjkR01/Hcv8jCK61uE9hbla1jc27fdUUjHp4MGfeHCbMGUo3HfOo+SQXdGmMhppBgxIhGv4jocrkg8zy8CF9zFiGP/UKW5Z7YfXQNX+fwgI5FpYdW+E8Yj0o0/h535dcKWv8I8yGzmXZ470wAhSx/ox3v8zytI+Y56Dx3JAWSkj+0Y9X/r6B8Env8+V7LzPJWdtX/vqjiNrF8CHbsHwrgAPPJDJ/yG+NsLFtoeqTtt+E9jDQqRECiJJCikwtGhctDOiIgrxQH/q82Yszv63Eo02drqoJgqbkt1kb9aVtBHadR5WSTl1orhmhgdrIiD/ix/4SDzzikpjps4QBbU7c9/k91I+tbdpGfWlPF8umLck0B4a/RdEm6ICVY2+65t2m8slt7JrrqvkAe6vyZ7Dxz4nOw1EDBgy0DIae0ICBVsbQCA0YaGX+D9+J/3JxHZm6AAAAAElFTkSuQmCC) est la
matrice de dispérsion intra-classe de la ième caractéristique par rapport à la k ième classe, et xij(k) désigne la valeur de la ième caractéristique pour le j ième échantillon de la k ième classe.

Bref, l'algorithme de score de Fisher sélectionne chaque caractéristique indépendamment en fonction de leurs scores.

Nous utilisons le module [skfeature-chappers](https://github.com/charliec443/scikit-feature) pour importer le package de score de fisher. 
"""

# Importer la bibliotheque permettant d'importer le score de fisher 
!pip install skfeature-chappers

# Commented out IPython magic to ensure Python compatibility.
## import des bibliotheques necessaires 
import pandas as pd
from skfeature.function.similarity_based import fisher_score
import matplotlib.pyplot as plt
# %matplotlib inline 
from sklearn.datasets import load_diabetes

# Charger l'ensemble des données
df= load_diabetes()
y= df.target
X= df.data

# Implementons les packages nécessaires
rank=fisher_score.fisher_score(X, y)
print(rank)
feat_importances=pd.Series(rank, df.feature_names[0:len(df.feature_names)])
feat_importances.plot(kind='bar', color='teal')
plt.show()

"""**3. Test du chi carré**

Ce [test ](https://towardsdatascience.com/chi-square-test-for-feature-selection-in-machine-learning-206b1f0b8223#:~:text=to%20normal%20distribution.-,Chi%2DSquare%20Test%20for%20Feature%20Selection,-A%20chi%2Dsquare)est utilisé dans le cadre de sélection des caractéristiques catégorielles. En effet, il suffit de calculer le chi-carré entre chaque feature et la feature target et nous sélectionnons ensuite les caractéristiques avec les meilleurs scores de chi-carré. Ce test exige certaines conditions:

- les variables doivent être catégorielles ;
- échantillonnées indépendamment ;
- et les valeurs doivent avoir une fréquence attendue supérieure à 5.

Étant donné les données de deux variables, nous pouvons obtenir le nombre observé O et le nombre attendu E. Le chi carré mesure comment le nombre attendu E et le nombre observé O s'écartent l'un de l'autre.

![image.png](data:image/png;base64,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)

Où 
c: est le degré de liberté
O: valeurs observées
E: valeurs attendues

Lorsque deux caractéristiques sont indépendantes, le nombre observé est proche du nombre attendu, nous aurons donc une valeur Chi-Square plus petite. Une valeur de chi carré si élevée indique que l'hypothèse d'indépendance est incorrecte. En termes simples, plus la valeur Chi-Square est élevée, plus la caractéristique dépend de la réponse et elle peut être sélectionnée pour la formation du modèle.


"""

# Import des bibliothéques nécessaires 
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# Charger les données
X, y= load_iris(return_X_y=True, as_frame=True)

print("Nombre de features initiales :", X.shape)

#Implémenter le packahe de Chi-Square
X_new = SelectKBest(chi2, k=2).fit_transform(X, y)
print("Nombre de features selectionnées :",X_new.shape)

"""**4. Coefficient de corrélation**

La [corrélation](http://www.biostat.ulg.ac.be/pages/Site_r/corr_pearson.html) permet de mesurer la relation linéaire entre 2 variables ou plus. Nous utilisons la corrélation pour la sélection de caractéristiques dans la logique que les variables pertinentes sont fortement corrélées avec la variable cible. En plus, il faut verifier que les variables ne sont pas correlées entre elles. En effet, si deux caractéristiques sont corrélées cela veut dire qu'on puisse prédire l'une à partir de l'autre. Par consequent, il est impératif de supprimer l'une de deux variables, car les deux variables n'ajoutent que la même information au modele et ainsi du bruit.
Nous utilisons la corrélation de Pearson qui est definie comme suit:

![image.png](data:image/png;base64,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)



"""

# Commented out IPython magic to ensure Python compatibility.
# Import des bibliothéques nécessaires
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.datasets import load_diabetes

#Charger les données
X, y = load_diabetes(return_X_y=True, as_frame=True)
df=pd.concat([X, y], axis=1)
df

# Matrice de corrélation
cor=df.corr()

#heatmap
plt.figure(figsize=(10,8))

sns.heatmap(cor, annot=True)

"""Une fois que nous avons les résultats de la corrélation, nous devons définir une valeur seuil pour la sélection des caractéristiques, disons 0,5 comme seuil de sélection des variables. En plus si nous constatons que deux variables indépendantes sont corrélées entre elles, alors nous pouvons supprimer dans ce cas la caractéristique qui a une valeur de coefficient de corrélation inferieure avec la vairable target. Bref, nous vérifions la corrélation entre toutes les caractéristiques, pour éviter une multicolinéarité.

**5. Seuil d'écart (Seuil de variance)**

Cette approche permet de supprimer toutes les features dont la variance n'atteint pas un certain seuil. Ce seuil est initialisé par défaut à zero (0)dans **[Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html)**. Il supprime alors toutes les caractéristiques à variance nulle. Cela implique les caractéristiques qui ont la même valeur dans tous les échantillons. Ces dérnieres sont constantes affichant des valeurs similaires/uniques dans toutes les observations du jeu de données. Ces caractéristiques ne fournissent aucune information pertinente permettant aux modeles d'apprentissage automatique de prédire la varible target. Par ailleurs, les caractéristiques avec une variance plus élevée peuvent contenir des informations plus utiles. Cette approche ne prend pas en compte la relation entre les caractéristiques ou les caractéristiques et la variable cible. 

Cette approche ne prend en compte que les caractéristiques (X), pas la variable cible (y), et peut donc être utilisé pour un apprentissage non supervisé.

- Si seuil de variance = 0 (supprimer les caractéristiques constantes)
- Si seuil de variance > 0 (supprimer les caractéristiques quasi-constantes)

Nous utilisans le seuil de variance de **Sklearn** pour supprimer toutes les caractéristiques affichant les mêmes valeurs.
"""

# Import des bibliothéques néecessaires
import pandas as pd
from sklearn.feature_selection import VarianceThreshold
from sklearn.datasets import load_diabetes

# Charger les données
X, y = load_diabetes(return_X_y=True, as_frame=True)

#Implementer le package de feature selection
v_threshold=VarianceThreshold(threshold=0)
v_threshold.fit_transform(X)
print(v_threshold.get_feature_names_out())
print()
print(v_threshold.get_params())
print()
print(v_threshold.get_support())

# Une autre maniére d'appliquer le seuil variance
from sklearn.feature_selection import VarianceThreshold
threshold_n=0
# Implementons le package
sel = VarianceThreshold(threshold=(threshold_n* (1 - threshold_n) ))
sel_var=sel.fit_transform(X)
#Affichons les variables selectionnées
X[X.columns[sel.get_support(indices=True)]]

"""- True: indique une faible variance
- False: indique une variance elevée

On constate qu'avec un seuil de zéro (0), nous conservons toutes les caractéristiques.

## **B.  Méthodes d'emballage (Wrapper Methods) :**

Les méthodes de Wrapper mesurent la pertinence des caractéristiques en fonction des pérformances du classiffier. C'est une approche gourmande qui évalue toutes les combinaisons possibles des caractéristiques par rapport au critère d'évaluation. Ce critère d'évaluation peut être la **p-values**, le **R au carré (R-squared)**, le **R au carré ajusté (Ajusted R-squared** dans un problème de régression. En revanche, dans un problème de classification, ce critère répresente **l'exactitude (accuracy)**, la **précision**, le **recall** et **f1-score**. 
Bref, ces méthodes sélectionnent la combinaison de caractéristiques qui donne les meilleurs resultats pour l'algorithme d'apprentissage automatique spécifié. 

Ces méthodes sont coûteuses en calcul par rapport aux méthodes de filtrage, mais elles surpassent généralement les méthodes de filtrage, car elles sélectionnent les caractéristiques en fonction des pérformances du prédicteur.

Nous presentons ici quelques méthodes de Wrapper

**1. Sélection des caracteristiques pas à pas vers l'avant ([Forward selection](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#:~:text=Done%20in%200.001s-,Selecting%20features%20with%20Sequential%20Feature%20Selection,-%C2%B6))**

Cette méthode de **forward selection** utilise la **régression linéaire** et une **valeur de p** pour la sélection des caractéristiques. La forward selection commence par régresser une caractéristique et si la caractéristique est significative selon la **valeur p**, cette caractéristique est **conservée**. Ce processus est répété en ajoutant une caractéristique à la fois jusqu'à ce que la liste des caractéristiques soit épuisée, et les caractéristiques significatives sont conservées et les caractéristiques non significatives sont supprimées.
"""

# Import des bibliothéques nécessaires
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_diabetes

#Charger les données
X, y = load_diabetes(return_X_y=True, as_frame=True)
feature_names = X.columns
reg = LinearRegression()

#Implémenter le package de feature selection
sfs = SequentialFeatureSelector(reg, direction = 'forward', n_features_to_select=4)
sfs.fit(X, y)

print()
print("Affichons les caractéristiques sélctionnées par cette méthode:"
f"{feature_names[sfs.get_support()].tolist()}")

"""**2. Élimination des caracteristiques pas à pas vers l'arrière ([back forward](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#:~:text=Done%20in%200.001s-,Selecting%20features%20with%20Sequential%20Feature%20Selection,-%C2%B6))**

Cette téchnique de **back forward** fonctionne dans le sens inverse de la **méthode forward**. Tout d'abord, on commence avec l'ensemble complet des caractéristiques et on élimine les caractéristiques avec des valeurs de **p-value** inférieure au **seuil de signification** choisi (en général, on choisit 5%). Ce processus se poursuit jusqu'à ce que toutes les caractéristiques sélctionnées auront une **p-value** supérieure au **seuil de signification**. 

"""

# Import des bibliothéques nécessaires 
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_diabetes

#Charger les données 
X, y = load_diabetes(return_X_y=True, as_frame=True)
feature_names = X.columns
reg = LinearRegression()

#Implementer le package
sfs = SequentialFeatureSelector(reg, direction = 'backward', n_features_to_select=4)
sfs.fit(X, y)

print()
print("Affichons les caractéristiques sélctionnées par cette méthode:"
f"{feature_names[sfs.get_support()].tolist()}")

"""**3. Élimination des caracteristiques récursives ([Recursive feature elimination ](https://machinelearningmastery.com/rfe-feature-selection-in-python/))**

Étant donné un estimateur externe qui attribue des poids aux caractéristiques (par exemple, les coefficients d'un modèle linéaire), l'objectif de l'élimination récursive des caractéristiques (RFE) est de rechercher un sous-ensemble de caractéristiques en commençant par toutes les caractéristiques de l'ensemble de données d'apprentissage et en supprimant avec succès les caractéristiques jusqu'à ce qu'il reste le nombre souhaité.
Il faut noter deux points dans RFE : 

- Tout d'abord, l'estimateur est formé sur l'ensemble complet de caractéristiques et l'importance de chaque caractéristique est obtenue soit via un attribut **coef_** soit via un attribut **feature_importances_**.

- Ensuite, les caractéristiques les moins importantes sont supprimées de l'ensemble de caractéristiques. Cette procédure est répétée de manière récursive sur l'ensemble élagué jusqu'à ce que le nombre souhaité de caractéristiques à sélectionner soit finalement atteint. 
"""

#Import des bibliothéques nécessaires 
from sklearn.feature_selection import RFE
from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression

# Charger les données
X, y = load_diabetes(return_X_y=True, as_frame=True)
feature_names = X.columns
estimator = LinearRegression()

#Implémentons le package
selector = RFE(estimator, n_features_to_select=5, step=1)
selector = selector.fit(X, y)
selector.support_


print(selector.ranking_)

print()

print("Le nombre optimal de features : %d" % selector.n_features_)
print()

"""**4. Selction des caracteristiques en fonction de leur importance**

La classe Sklearn **SelectFromModel** permet de réaliser la sélection des caractéristiques selon une valeur d'importance supérieure à une valeur seuil spécifiée dans un classificateur ([par exemple ridge](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#:~:text=S%C3%A9lection%20de%20fonctionnalit%C3%A9s%20en%20fonction%20de%20leur%20importance)). 

Les étapes suivantes montrent comment peut-on utiliser la classe **SelectFromModel :**

- 1. Tout d'abord, déterminer l'importance de la caractéristique à l'aide d'un estimateur tel que la **Régression logistique**, **[RandomForestClassifier](https://vitalflux.com/sklearn-selectfrommodel-feature-importance-python/#:~:text=Sklearn%20SelectFromModel%20pour%20l%27importance%20des%20fonctionnalit%C3%A9s)**, **Ridge**... 

- 2. Créeation d'un estimateur à l'aide de la classe **SelectFromModel** qui prend des paramètres tels que l'estimateur déjà spécifié (**par exemple régression logistique ou RandomClassifier**) ​​et le **seuil**

- 4. Créer le tracé de visualisation représentant les caractéristiques 


**Exemple**

Nous allons utiliser un exemple de **Sklearn** pour mieux éclaircir cette méthdoe de sélection de caractéristiques via leur importance. Nous utilisons l'estimateur **RidgeCVestimateur**. Les caractéristiques dont la valeur absolue de **coef_** la plus **élevée** sont **considérées** comme **les plus importantes**. Il est utile de normaliser les caractéristiques avant d'utilider cette méthode. Les données que nous utiulisons dans cet exemple sont dèjà [normalisées](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#:~:text=the%20features%20were%20already%20standardized). 
"""

# Import des bibliothéques nécessaires 
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import RidgeCV
from sklearn.datasets import load_diabetes

# Charger les données
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target
print(diabetes.DESCR)

# Implémentons le modéle Ridge
ridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(X, y)
importance = np.abs(ridge.coef_)
feature_names = np.array(diabetes.feature_names)
plt.bar(height=importance, x=feature_names)
plt.title("Feature importances via coefficients")
plt.show()
print()

print("Valeurs des coefficients des caracteristiques :", importance)

print()

#Implémentons le méthode de feature selection
from sklearn.feature_selection import SelectFromModel
from time import time

threshold = np.sort(importance)[-3] + 0.01

print("Le seuil de selection de caracteristiques :", threshold)

print()

tic = time()
sfm = SelectFromModel(ridge, threshold=threshold).fit(X, y)
toc = time()
print(f"Les caracteristiques selectionnées par SelectFromModel: {feature_names[sfm.get_support()]}")
print(f"Fait en {toc - tic:.3f}s")

"""**5. algorithme génétique**

La méthode d'**algorithme génétique** est une méthode inspirée de la **sélection naturelle** et généralement basée sur un ensemble de fonctions telles que la **mutation**, le **croisement** et la **sélection**. Ces trois phases sont appelées en général des opérateurs génétiques.

Il existe plusieurs variantes de cette méthode, mais en général, les étapes à suivre ressemblent à ceci :

- 1. Tout d'abord, générer une **population** échantillonnée au hasard basée sur des sous-ensemble des caractéristiques possibles ; c'est la **génération 0**.
- 2. Évaluer la valeur de **fitness** de chaque  sous-ensemble des caractéristiques à l'aide d'un modéle prédictif, obtener les scores de validation croisée.
- 3. Générer une nouvelle **génération** avec un sous-ensemble des caractéristiques jugées importantes.
- 4. Répéter les étapes 2 et 3 jusqu'à ce qu'on rencontre un **critère d'arrêt**.

Le critère d'arrêt pourrait être : Un **nombre maximum** de **générations** a été atteint.

Il faut savoir que l'**algorithme génétique** peut également être utilisé pour l'optimisation des **hyperparamètres**.

Bref nous resumons les étapes ci-dessous :

- Tout d'abord, une population initiale est produite.
- Ensuite, un score est attaché aux membres de la population.
- Et un sous-ensemble est sélectionné pour être réproduit avec un tournoi.
- Sélectionner ensuite le matériel génétique à transmettre.
- Appliquer des mutations.
- Répéter sur plusieurs générations.

L'algorithme s'exécute pendant un nombre défini de générations (itérations). Après quoi, le membre optimal de la population sont les caractéristiques sélectionnées.
"""

# Nous utilisons ce package pour implementer cet algorithme génétique
!pip install sklearn-genetic-opt[all]

"""**Exemple de sélection de caractéristiques via l'algorithme génétique**

Pour cet exemple, le jeu de données Iris est utilisé, il s'agit d'un problème de classification avec quatre caractéristiques. Un bruit aléatoire a été simulé pour représenter des caractéristiques non importantes afin de mieux démontrer la sélection des caractéristiques.

Avant de commencer, presentons tout d'abord les packages de l'implementation de l'algorithme.

**Présentation de Sklearn-genetic-opt**

**Sklearn-genetic-opt** est un package basé sur Python qui utilise des algorithmes évolutifs du package [DEAP](https://deap.readthedocs.io/en/master/) pour choisir l'ensemble d'**hyperparamètres** qui optimise (max ou min) les **scores** de validation croisée ou utiliser pour la sélection des caractéristiques; le package peut être utilisé à la fois pour les problèmes de **régression** et de **classification**. À ce stade, **Sklearn-genetic-opt** est compatible avec n'importe quel **régresseur** ou **classificateur** **scikit-learn**.

**GAFeatureSelectionCV :** 

Cette classe complémentaire du package éffectue la sélection de caractéristiques tout en optimisant les **cv-scores**, tout cela, avec un ensemble **fixe** d'**hyperparamètres**.
"""

#Import des bibliothéques nécessaires
import matplotlib.pyplot as plt
from sklearn_genetic import GAFeatureSelectionCV
from sklearn_genetic.plots import plot_fitness_evolution
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.svm import SVC
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score
import numpy as np

# Charger les données
data = load_iris()
X, y = data["data"], data["target"]

noise = np.random.uniform(0, 10, size=(X.shape[0], 10))

X = np.hstack((X, noise))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)

"""10 caractéristiques bruyantes supplémentaires ont été crées dans notre ensemble d'entraînement et de test.

Création de l'objet de sélection de caractéristiques, l'estimateur que nous allons utiliser est un **Support Vector Machine** (SVM) :
"""

clf = SVC(gamma='auto')

evolved_estimator = GAFeatureSelectionCV(
    estimator=clf,
    cv=3,
    scoring="accuracy",
    population_size=30,
    generations=20,
    n_jobs=-1,
    verbose=True,
    keep_top_k=2,
    elitism=True,
)

"""Ajustons la méthode aux données :"""

# Train and select the features
evolved_estimator.fit(X_train, y_train)

"""Ce **journal**, montre les métriques obtenues à chaque itération (génération), voici ce que signifie chaque entrée :

**gen** : Le numéro de la génération

**nevals** : combien d'hyperparamètres ont été ajustés dans cette génération

**fitness** : mesure du score moyen dans la validation croisée (ensemble de validation). Dans ce cas, la précision moyenne sur les plis de tous les ensembles d'hyperparamètres.

**fitness_std** : écart type de la précision des validations croisées.

**fitness_max** : Le score individuel maximum de tous les modèles de cette génération.

**fitness_min** : Le score individuel minimum de tous les modèles de cette génération.

Après avoir ajusté le modèle, nous utiliserons les methodes suivantes pour voir si notre modéle à performer après suppression de quelques caractéristiques. Cette méthode génétique utilise par défaut le meilleur ensemble de caractéristiques qu'il a trouvé.
"""

# Stackage de sous-ensemble de caracteristiques sélectionnées
features = evolved_estimator.best_features_

# Predict only with the subset of selected features
y_predict_ga = evolved_estimator.predict(X_test[:, features])
accuracy = accuracy_score(y_test, y_predict_ga)
accuracy

print(evolved_estimator.best_features_)
print("accuracy_score:", "{:.2f}".format(accuracy))

"""Dans ce cas, nous avons obtenu un score de précision dans l'ensemble de test de 0,98.

Notez que le **best_features_** est un vecteur de valeurs booléennes, chaque position représente l'index de la caractéristique (colonne) et la valeur indique si cette caractéristique a été sélectionnée (**True**) ou non (**False**) par l'algorithme. Dans cet exemple, l'algorithme a rejeté toutes les variables aléatoires bruitées que nous avons créées et a sélectionné les variables d'origine.
"""

#Nous pouvons également tracer l'évolution de la condition physique :

from sklearn_genetic.plots import plot_fitness_evolution
plot_fitness_evolution(evolved_estimator)
plt.show()

"""## **C.  Méthodes intégrées (Embedded Methods):**

Ces méthodes englobent les avantages des méthodes de Wrapper et de filtrage, en incluant les interactions des caractéristiques, mais également en maintenant un coût de calcul raisonnable. Les méthodes intégrées sont itératives en ce sens qu'elles prennent en charge chaque itération du processus de formation du modèle et extraient soigneusement les caractéristiques qui contribuent le plus.

**1. Régularisation LASSO (L1)**

Cette méthode de régularisation consiste à ajouter une pénalité aux différents paramètres (coefficients) du classificateur pour contracter la valeur des coefficients, c'est-à-dire éviter le sur-ajustement. Dans la régularisation du modèle linéaire, la pénalité est appliquée sur les coefficients qui multiplient chacun des prédicteurs. Parmi les différents types de régularisation, Lasso ou L1 a la propriété de réduire certains coefficients à zéro. Par conséquent, cette caractéristique correspondante peut être supprimée du modèle.
"""

#Import des bibliothéques nécessaires
import matplotlib.pyplot as plt
import numpy as np
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LassoCV

from sklearn.datasets import load_diabetes

# Charger les données
diabetes = load_diabetes()
X, y = load_diabetes(return_X_y=True, as_frame=True)
feature_names = X.columns

reg=LinearRegression()
#lasso=LassoCV()

#Implémenter la selection des caractéristiques
selector = SelectFromModel(reg)
selector.fit(X,y)
selector.estimator_.coef_

print(selector.threshold_)
print()

print(selector.get_support())
print()

print(selector.transform(X))

print()
print("Les caractéristiques sélectionnées:"
f"{feature_names[selector.get_support()].tolist()}")

"""**2. La forêt aléatoire (RandomForest)**

Cet algorithme de Random Forests est un **algorithme de Bagging** (bootstrap aggregating) qui agrège un nombre spécifié d'arbres de décision. Les stratégies arborescentes utilisées par les forêts aléatoires se classent naturellement en fonction de leur degré d'amélioration de la pureté du nœud, ou en d'autres termes d'une diminution de l'impureté (**impureté Gini**) sur tous les arbres. Les nœuds avec la plus grande diminution d'impureté se produisent au début des arbres, tandis que les nœuds avec la moindre diminution d'impureté se produisent à la fin des arbres. Ainsi, en élaguant les arbres sous un nœud particulier, nous pouvons créer un sous-ensemble des caractéristiques les plus importantes.

L'exemple ci-dessous existe [ici](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)
"""

# Import des bibliothéques nécessaires
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_diabetes

# Charger les données
diabetes = load_diabetes()
X, y = load_diabetes(return_X_y=True, as_frame=True)

#Implémentons le modéle RandomForedt
model=RandomForestClassifier()

#Ajustons le modéle
model.fit(X,y)

#Les features selon leur importance
features_importance=model.feature_importances_

features_importance

# Une liste connent les noms de colonnes
feat_labels = ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']

# Affichons les colonnes avec leur valeur d'importance dans RandomForest
for feature in zip(feat_labels, model.feature_importances_):
    print(feature)

import time
import numpy as np

start_time = time.time()
importances = model.feature_importances_
std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)
elapsed_time = time.time() - start_time

print(f"Temps écoulé pour calculer les importances: {elapsed_time:.3f} seconds")

# Affichopns les features selon leur importance
import pandas as pd

forest_importances = pd.Series(importances, index=feature_names)

fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=std, ax=ax)
ax.set_title("Feature importances using MDI")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()

# On va utiliser S electFromModel avec un seuil de 0.109 pour la selction de caractéristiques via RandomForest
sfm = SelectFromModel(model, threshold=0.109)

# Ajustement 
sfm.fit(X, y)

# Affichons les caractéristiques importantes 
for feature_list_index in sfm.get_support(indices=True):
    print(feat_labels[feature_list_index])

"""## **Conclusion**

Nous vennons de voir certaines méthodes de séléction des caracteristiques ou du moins les plus importantes selon les recherches que nous avons pu faire. Les méthodes integrées sont le plus conseillées car elles combinent les avantages de méthodes de filtrage et de Wrapper. Il existe plusieurs d'autres méthodes que nous avons pas cités ici. Du coup vous pourriez faire votre propre recherche pour compléter les méthodes que nous avons présenter ici. Il est crucial de connaitre ces téchniques de sélection de caractéristiques, car elles permettent de réduire la variance et ainsi le surajustemnt et permettent également de réduire le coût de calcul, ce qui réduira le temps de formation du modéle.  

Nous avons juste regrouper les méthodes pour une future utilisation, nous avons rien inventé. Tout est dans les références. J'ai opté pour une présentation en français car la plus part de recherches ou d'articles sont rédigés enb anglais. Du coup, je me suis dit pourquoi une présentation en français. 

J'espere que cela pourrait servir certains.

#### **References**



1. [Feature Selection Techniques in Machine Learning](https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/)


3. [Sélection des fonctionnalités et ingénierie des fonctionnalités](https://www.sciencedirect.com/science/article/pii/S1568494620302039?casa_token=s_9NH3dSfjYAAAAA:T14JmdtufUrWqbLu_H-v8MRSUl7ByJYuvUxS_xDPfPKGoj5R5Qc_QCCqAxLxN4tIpiR7CQVkgg)


4. [Sélection de fonctionnalités à l'aide d'un algorithme génétique en Python](https://radhajayaraman11.medium.com/feature-selection-using-genetic-algorithm-2f915d1349b0)

5. [Feature Selection with Genetic Algorithms](https://towardsdatascience.com/feature-selection-with-genetic-algorithms-7dd7e02dd237)

6. [Comment utiliser Sklearn-genetic-opt](https://sklearn-genetic-opt.readthedocs.io/en/stable/tutorials/basic_usage.html)

7. [texte du lien](https://towardsdatascience.com/hyperparameters-tuning-from-grid-search-to-optimization-a09853e4e9b8#:~:text=4.%20Approche%20des%20algorithmes%20g%C3%A9n%C3%A9tiques%20(GA))

8. [Github 1 sklearn-genetic](https://github.com/rodrigo-arenas/Sklearn-genetic-opt)

9. [Github 2 sklearn-genetic](https://github.com/manuel-calzolari/sklearn-genetic)
"""